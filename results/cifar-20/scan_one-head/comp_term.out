XAI-SCAN> python scan.py --config_env configs/env.yml --config_exp configs/scan/scan_cifar20.yml
{'setup': 'scan', 'criterion': 'scan', 'criterion_kwargs': {'entropy_weight': 5.0}, 'update_cluster_head_only': False, 'num_heads': 1, 'backbone': 'resnet18', 'train_db_name': 'cifar-20', 'val_db_name': 'cifar-20', 'num_classes': 20, 'num_neighbors': 20, 'augmentation_strategy': 'scan', 'augmentation_kwargs': {'crop_size': 32, 'normalize': {'mean': [0.5071, 0.4867, 0.4408], 'std': [0.2675, 0.2565, 0.2761]}, 'num_strong_augs': 4, 'cutout_kwargs': {'n_holes': 1, 'length': 16, 'random': True}}, 'transformation_kwargs': {'crop_size': 32, 'normalize': {'mean': [0.5071, 0.4867, 0.4408], 'std': [0.2675, 0.2565, 0.2761]}}, 'optimizer': 'adam', 'optimizer_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}, 'epochs': 100, 'batch_size': 512, 'num_workers': 8, 'scheduler': 'constant', 'pretext_dir': './results\\cifar-20\\pretext', 'pretext_checkpoint': './results\\cifar-20\\pretext\\checkpoint.pth.tar', 'pretext_model': './results\\cifar-20\\pretext\\model.pth.tar', 'topk_neighbors_train_path': './results\\cifar-20\\pretext\\topk-train-neighbors.npy', 'topk_neighbors_val_path': './results\\cifar-20\\pretext\\topk-val-neighbors.npy', 'scan_dir': './results\\cifar-20\\scan', 'scan_checkpoint': './results\\cifar-20\\scan\\checkpoint.pth.tar', 'scan_model': './results\\cifar-20\\scan\\model.pth.tar', 'selflabel_dir': './results\\cifar-20\\selflabel', 'selflabel_checkpoint': './results\\cifar-20\\selflabel\\checkpoint.pth.tar', 'selflabel_model': './results\\cifar-20\\selflabel\\model.pth.tar'}
Get dataset and dataloaders
Files already downloaded and verified
Files already downloaded and verified
Train transforms: Compose(
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(32, 32), padding=None)
    <data.augment.Augment object at 0x000002450D53B9A0>
    ToTensor()
    Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
    <data.augment.Cutout object at 0x000002450D53B910>
)
Validation transforms: Compose(
    CenterCrop(size=(32, 32))
    ToTensor()
    Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
)
Train samples 50000 - Val samples 10000
Get model
ClusteringModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (shortcut): Sequential()
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  )
  (cluster_head): ModuleList(
    (0): Linear(in_features=512, out_features=20, bias=True)
  )
)
Get optimizer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001
)
Get loss
SCANLoss(
  (softmax): Softmax(dim=1)
  (bce): BCELoss()
)
No checkpoint file at ./results\cifar-20\scan\checkpoint.pth.tar
Starting main loop
Epoch 1/100
---------------
Adjusted learning rate to 0.00010
Train ...
Epoch: [0][ 0/97]       Total Loss -1.1982e+01 (-1.1982e+01)    Consistency Loss 2.9944e+00 (2.9944e+00)   Entropy 2.9953e+00 (2.9953e+00)
Epoch: [0][25/97]       Total Loss -1.2002e+01 (-1.1991e+01)    Consistency Loss 2.9710e+00 (2.9867e+00)        Entropy 2.9946e+00 (2.9955e+00)
Epoch: [0][50/97]       Total Loss -1.2102e+01 (-1.2021e+01)    Consistency Loss 2.8499e+00 (2.9487e+00)     Entropy 2.9904e+00 (2.9940e+00)
Epoch: [0][75/97]       Total Loss -1.2216e+01 (-1.2060e+01)    Consistency Loss 2.7109e+00 (2.9007e+00)   Entropy 2.9855e+00 (2.9921e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.982285261154175, 'consistency': 2.5575456619262695, 'total_loss': -0.4247395992279053}], 'lowest_loss_head': 0, 'lowest_loss': -0.4247395992279053}
New lowest loss on validation set: 10000.0000 -> -0.4247
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.2303, 'ARI': 0.12589017123905588, 'NMI': 0.22657533474627875, 'ACC Top-5': 0.5759, 'hungarian_match': [(0, 18), (1, 5), (2, 3), (3, 8), (4, 4), (5, 0), (6, 15), (7, 10), (8, 6), (9, 7), (10, 1), (11, 19), (12, 12), (13, 13), (14, 11), (15, 16), (16, 9), (17, 17), (18, 14), (19, 2)]}
Checkpoint ...
Epoch 2/100
---------------
Adjusted learning rate to 0.00010
Train ...
Epoch: [1][ 0/97]       Total Loss -1.2246e+01 (-1.2246e+01)    Consistency Loss 2.6905e+00 (2.6905e+00)   Entropy 2.9872e+00 (2.9872e+00)
Epoch: [1][25/97]       Total Loss -1.2336e+01 (-1.2295e+01)    Consistency Loss 2.5925e+00 (2.6357e+00)   Entropy 2.9857e+00 (2.9861e+00)
Epoch: [1][50/97]       Total Loss -1.2457e+01 (-1.2343e+01)    Consistency Loss 2.4849e+00 (2.5897e+00)   Entropy 2.9883e+00 (2.9865e+00)
Epoch: [1][75/97]       Total Loss -1.2571e+01 (-1.2396e+01)    Consistency Loss 2.3727e+00 (2.5400e+00)   Entropy 2.9887e+00 (2.9871e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.9813003540039062, 'consistency': 2.0210154056549072, 'total_loss': -0.960284948348999}], 'lowest_loss_head': 0, 'lowest_loss': -0.960284948348999}
New lowest loss on validation set: -0.4247 -> -0.9603
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.2537, 'ARI': 0.1466866689473708, 'NMI': 0.2661971109324486, 'ACC Top-5': 0.5762, 'hungarian_match': [(0, 19), (1, 18), (2, 5), (3, 8), (4, 3), (5, 0), (6, 12), (7, 10), (8, 6), (9, 13), (10, 1), (11, 16), (12, 2), (13, 4), (14, 11), (15, 15), (16, 9), (17, 17), (18, 14), (19, 7)]}
Checkpoint ...
Epoch 3/100
---------------
Adjusted learning rate to 0.00010
Train ...
Epoch: [2][ 0/97]       Total Loss -1.2604e+01 (-1.2604e+01)    Consistency Loss 2.3552e+00 (2.3552e+00)   Entropy 2.9919e+00 (2.9919e+00)
Epoch: [2][25/97]       Total Loss -1.2709e+01 (-1.2693e+01)    Consistency Loss 2.2456e+00 (2.2627e+00)   Entropy 2.9908e+00 (2.9910e+00)
Epoch: [2][50/97]       Total Loss -1.2753e+01 (-1.2743e+01)    Consistency Loss 2.2080e+00 (2.2145e+00)   Entropy 2.9922e+00 (2.9914e+00)
Epoch: [2][75/97]       Total Loss -1.2939e+01 (-1.2785e+01)    Consistency Loss 2.0205e+00 (2.1735e+00)   Entropy 2.9920e+00 (2.9917e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.9723293781280518, 'consistency': 1.6027148962020874, 'total_loss': -1.3696144819259644}], 'lowest_loss_head': 0, 'lowest_loss': -1.3696144819259644}
New lowest loss on validation set: -0.9603 -> -1.3696
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.2726, 'ARI': 0.1658906752049761, 'NMI': 0.3060130489392228, 'ACC Top-5': 0.6163, 'hungarian_match': [(0, 19), (1, 13), (2, 5), (3, 8), (4, 9), (5, 0), (6, 7), (7, 10), (8, 3), (9, 2), (10, 15), (11, 1), (12, 16), (13, 4), (14, 18), (15, 11), (16, 6), (17, 17), (18, 14), (19, 12)]}
Checkpoint ...

.
..
...

Epoch 98/100
---------------
Adjusted learning rate to 0.00010
Train ...
Epoch: [97][ 0/97]      Total Loss -1.3872e+01 (-1.3872e+01)    Consistency Loss 1.0762e+00 (1.0762e+00)   Entropy 2.9897e+00 (2.9897e+00)
Epoch: [97][25/97]      Total Loss -1.3881e+01 (-1.3933e+01)    Consistency Loss 1.0642e+00 (1.0117e+00)   Entropy 2.9890e+00 (2.9889e+00)
Epoch: [97][50/97]      Total Loss -1.3967e+01 (-1.3924e+01)    Consistency Loss 9.6644e-01 (1.0179e+00)   Entropy 2.9866e+00 (2.9884e+00)
Epoch: [97][75/97]      Total Loss -1.3900e+01 (-1.3924e+01)    Consistency Loss 1.0454e+00 (1.0201e+00)   Entropy 2.9890e+00 (2.9889e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.9910213947296143, 'consistency': 0.8830700516700745, 'total_loss': -2.10795134305954}], 'lowest_loss_head': 0, 'lowest_loss': -2.10795134305954}
No new lowest loss on validation set: -2.1198 -> -2.1080
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.4079, 'ARI': 0.2680188045279671, 'NMI': 0.43972458643172335, 'ACC Top-5': 0.7357, 'hungarian_match': [(0, 18), (1, 1), (2, 5), (3, 8), (4, 7), (5, 0), (6, 13), (7, 10), (8, 9), (9, 2), (10, 12), (11, 3), (12, 16), (13, 14), (14, 19), (15, 15), (16, 6), (17, 17), (18, 4), (19, 11)]}
Checkpoint ...
Epoch 99/100
---------------
Adjusted learning rate to 0.00010
Epoch: [98][ 0/97]      Total Loss -1.3930e+01 (-1.3930e+01)    Consistency Loss 1.0182e+00 (1.0182e+00Entropy 2.9897e+00 (2.9897e+00)
Epoch: [98][25/97]      Total Loss -1.3864e+01 (-1.3917e+01)    Consistency Loss 1.0854e+00 (1.0307e+00Entropy 2.9899e+00 (2.9895e+00)
Epoch: [98][50/97]      Total Loss -1.3881e+01 (-1.3924e+01)    Consistency Loss 1.0674e+00 (1.0232e+00Entropy 2.9897e+00 (2.9894e+00)
Epoch: [98][75/97]      Total Loss -1.3938e+01 (-1.3922e+01)    Consistency Loss 1.0241e+00 (1.0229e+00Entropy 2.9925e+00 (2.9890e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.9929001331329346, 'consistency': 0.8785805106163025, 'total_loss': -2.114319622516632}], 'lowest_loss_head': 0, 'lowest_loss': -2.114319622516632}
No new lowest loss on validation set: -2.1198 -> -2.1143
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.4069, 'ARI': 0.2678729166537625, 'NMI': 0.4438302458576692, 'ACC Top-5': 0.7384, 'hungarian_match': [(0, 18), (1, 1), (2, 5), (3, 8), (4, 7), (5, 0), (6, 15), (7, 10), (8, 9), (9, 2), (10, 12), (11, 3), (12, 16), (13, 14), (14, 19), (15, 13), (16, 6), (17, 17), (18, 4), (19, 11)]}
Checkpoint ...
Epoch 100/100
---------------
Adjusted learning rate to 0.00010
Train ...
Epoch: [99][ 0/97]      Total Loss -1.3933e+01 (-1.3933e+01)    Consistency Loss 1.0030e+00 (1.0030e+00Entropy 2.9872e+00 (2.9872e+00)
Epoch: [99][25/97]      Total Loss -1.3992e+01 (-1.3930e+01)    Consistency Loss 9.4457e-01 (1.0133e+00Entropy 2.9874e+00 (2.9887e+00)
Epoch: [99][50/97]      Total Loss -1.3894e+01 (-1.3920e+01)    Consistency Loss 1.0643e+00 (1.0241e+00Entropy 2.9916e+00 (2.9887e+00)
Epoch: [99][75/97]      Total Loss -1.3952e+01 (-1.3923e+01)    Consistency Loss 9.9880e-01 (1.0224e+00Entropy 2.9902e+00 (2.9890e+00)
Make prediction on validation set ...
Evaluate based on SCAN loss ...
{'scan': [{'entropy': 2.9908533096313477, 'consistency': 0.8686630129814148, 'total_loss': -2.122190296649933}], 'lowest_loss_head': 0, 'lowest_loss': -2.122190296649933}
New lowest loss on validation set: -2.1198 -> -2.1222
Lowest loss head is 0
Evaluate with hungarian matching algorithm ...
{'ACC': 0.4101, 'ARI': 0.2697134921076171, 'NMI': 0.44244751274291744, 'ACC Top-5': 0.7394, 'hungarian_match': [(0, 18), (1, 1), (2, 5), (3, 8), (4, 7), (5, 0), (6, 13), (7, 10), (8, 9), (9, 2), (10, 12), (11, 3), (12, 16), (13, 14), (14, 19), (15, 15), (16, 6), (17, 17), (18, 4), (19, 11)]}
Checkpoint ...
Evaluate best model based on SCAN metric at the end
{'ACC': 0.4101, 'ARI': 0.2697134921076171, 'NMI': 0.44244751274291744, 'ACC Top-5': 0.7394, 'hungarian_match': [(0, 18), (1, 1), (2, 5), (3, 8), (4, 7), (5, 0), (6, 13), (7, 10), (8, 9), (9, 2), (10, 12), (11, 3), (12, 16), (13, 14), (14, 19), (15, 15), (16, 6), (17, 17), (18, 4), (19, 11)]}